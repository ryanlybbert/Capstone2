{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14770 entries, c1e4o to c0i10ti\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   score             14770 non-null  int64         \n",
      " 1   controversiality  14770 non-null  int64         \n",
      " 2   subreddit         14770 non-null  object        \n",
      " 3   body              14770 non-null  object        \n",
      " 4   month             14770 non-null  int64         \n",
      " 5   year              14770 non-null  int64         \n",
      " 6   original_size     14770 non-null  int64         \n",
      " 7   PS                14770 non-null  int64         \n",
      " 8   XBOX              14770 non-null  int64         \n",
      " 9   PS_Count          14770 non-null  float64       \n",
      " 10  XBOX_Count        14770 non-null  float64       \n",
      " 11  date              14770 non-null  datetime64[ns]\n",
      " 12  naive_sentiment   14770 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int64(7), object(2)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_json('dataframes/reddit_data.json')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we'll be needing a value to predict.  I'm going to be using a secondary sentiment analysis through a bert classifier that ranks the sentiment on a scale of 1-5 stars, as well as gives a confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since this is a lot of data to keep in memory at once, I'm splitting up the work so I can save progress over time\n",
    "df06 = df[df['year']==2006]\n",
    "df07 = df[df['year']==2007]\n",
    "df08 = df[df['year']==2008]\n",
    "df09 = df[df['year']==2009]\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2006\n",
    "labels = []\n",
    "scores = []\n",
    "for post in tqdm(df06['body']):\n",
    "        a = classifier(post, max_length=max_length, truncation=True) #this returns a 1-element list of a dictionary\n",
    "        labels.append(a[0]['label'][0]) #a[0]['label'] returns a label between 1 star and 5 stars, I only need the number\n",
    "        scores.append(a[0]['score']) #confidence score of said label\n",
    "df06['bert_labels'] = labels\n",
    "df06['bert_scores'] = scores\n",
    "\n",
    "print(df06.head())\n",
    "df06.to_json('dataframes/df06_bert.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2007\n",
    "labels = []\n",
    "scores = []\n",
    "for post in tqdm(df07['body']):\n",
    "        a = classifier(post, max_length=max_length, truncation=True)\n",
    "        labels.append(a[0]['label'][0])\n",
    "        scores.append(a[0]['score'])\n",
    "df07['bert_labels'] = labels\n",
    "df07['bert_scores'] = scores\n",
    "\n",
    "print(df07.head())\n",
    "df07.to_json('dataframes/df07_bert.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2008\n",
    "labels = []\n",
    "scores = []\n",
    "for post in tqdm(df08['body']):\n",
    "        a = classifier(post, max_length=max_length, truncation=True)\n",
    "        labels.append(a[0]['label'][0])\n",
    "        scores.append(a[0]['score'])\n",
    "df08['bert_labels'] = labels\n",
    "df08['bert_scores'] = scores\n",
    "\n",
    "print(df08.head())\n",
    "df08.to_json('dataframes/df08_bert.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2009\n",
    "labels = []\n",
    "scores = []\n",
    "for post in tqdm(df09['body']):\n",
    "        a = classifier(post, max_length=max_length, truncation=True)\n",
    "        labels.append(a[0]['label'][0])\n",
    "        scores.append(a[0]['score'])\n",
    "df09['bert_labels'] = labels\n",
    "df09['bert_scores'] = scores\n",
    "\n",
    "print(df09.head())\n",
    "df09.to_json('dataframes/df09_bert.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14770 entries, c1e4o to c0i10ti\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   score             14770 non-null  int64         \n",
      " 1   controversiality  14770 non-null  int64         \n",
      " 2   subreddit         14770 non-null  object        \n",
      " 3   body              14770 non-null  object        \n",
      " 4   month             14770 non-null  int64         \n",
      " 5   year              14770 non-null  int64         \n",
      " 6   original_size     14770 non-null  int64         \n",
      " 7   PS                14770 non-null  int64         \n",
      " 8   XBOX              14770 non-null  int64         \n",
      " 9   PS_Count          14770 non-null  float64       \n",
      " 10  XBOX_Count        14770 non-null  float64       \n",
      " 11  date              14770 non-null  datetime64[ns]\n",
      " 12  naive_sentiment   14770 non-null  float64       \n",
      " 13  bert_labels       14770 non-null  int64         \n",
      " 14  bert_scores       14770 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(4), int64(8), object(2)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#recombining data after the split\n",
    "df06 = pd.read_json('dataframes/df06_bert.json')\n",
    "df07 = pd.read_json('dataframes/df07_bert.json')\n",
    "df08 = pd.read_json('dataframes/df08_bert.json')\n",
    "df09 = pd.read_json('dataframes/df09_bert.json')\n",
    "\n",
    "df = df06.append(df07.append(df08.append(df09)))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing I need to do is decide which columns will be useful for my model.\n",
    "\n",
    "score should have an impact, though controversiality might not since it's effectively a boolean value with a large majority of values being equal to 0.\n",
    "body will definitely make an impact, although it should have dummies for popular terms.\n",
    "month, year, and date are mostly for the time series I made previously, and will not add much.\n",
    "original_size, PS_Count, and XBOX_Count were columns I added for plotting, and have no purpose here.\n",
    "naive_sentiment is definitely important.\n",
    "PS, XBOX, and subreddit are categoricals that should aid in predictions.\n",
    "bert_labels are going to be the prediction value and bert_scores will supplement that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14770 entries, c1e4o to c0i10ti\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   score            14770 non-null  int64  \n",
      " 1   subreddit        14770 non-null  object \n",
      " 2   body             14770 non-null  object \n",
      " 3   PS               14770 non-null  int64  \n",
      " 4   XBOX             14770 non-null  int64  \n",
      " 5   naive_sentiment  14770 non-null  float64\n",
      " 6   bert_labels      14770 non-null  int64  \n",
      " 7   bert_scores      14770 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['PS_Count', 'controversiality', 'XBOX_Count', 'original_size', 'month', 'year', 'date'], inplace=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to address the score column's outliers.  Half of all values are between 0 and 3, but the overall range is from -45 to 398.  The graph from the EDA shows that the variance has been increasing over time and has a non-normal distribution.  If we only keep rows between -30 and 30, we only lose 161 columns in the process, and in doing so make score a more reliable predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              score            PS          XBOX  naive_sentiment  \\\n",
      "count  14609.000000  14609.000000  14609.000000     14609.000000   \n",
      "mean       2.132453      0.448080      0.610446         0.077496   \n",
      "std        4.001580      0.497314      0.487666         0.221732   \n",
      "min      -30.000000      0.000000      0.000000        -1.000000   \n",
      "25%        1.000000      0.000000      0.000000        -0.006667   \n",
      "50%        1.000000      0.000000      1.000000         0.056481   \n",
      "75%        3.000000      1.000000      1.000000         0.194924   \n",
      "max       30.000000      1.000000      1.000000         1.000000   \n",
      "\n",
      "        bert_labels   bert_scores  \n",
      "count  14609.000000  14609.000000  \n",
      "mean       2.486412      0.436582  \n",
      "std        1.465876      0.145641  \n",
      "min        1.000000      0.208104  \n",
      "25%        1.000000      0.329384  \n",
      "50%        2.000000      0.401645  \n",
      "75%        4.000000      0.508862  \n",
      "max        5.000000      0.980305  \n"
     ]
    }
   ],
   "source": [
    "df.drop(df[(df['score'] > 30) | (df['score'] < -30)].index, inplace=True)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, this dropped the std of score from 11.1 to 4.001, while maintaining the percentiles and only changing the mean by about 0.8.  The last thing we need to address is the body and subreddit columns.  We should be able to create dummies for subreddit just fine, but the sheer number of unique words in body would add almost 20,000 columns to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 5842\n",
      "body 0\n",
      "PS 6546\n",
      "XBOX 8918\n",
      "naive_sentiment 51\n",
      "bert_labels 5671\n",
      "bert_scores 0\n",
      "subreddit_4chan 1\n",
      "subreddit_AmericanGovernment 1\n",
      "subreddit_AmericanPolitics 1\n",
      "subreddit_Anarchism 5\n",
      "subreddit_Android 41\n",
      "subreddit_Art 1\n",
      "subreddit_AskReddit 1250\n",
      "subreddit_Astronomy 1\n",
      "subreddit_BDSMcommunity 1\n",
      "subreddit_Baking 1\n",
      "subreddit_Borderlands 6\n",
      "subreddit_Christianity 2\n",
      "subreddit_CommonLaw 2\n",
      "subreddit_DAE 2\n",
      "subreddit_DIY 2\n",
      "subreddit_Design 1\n",
      "subreddit_DoesAnybodyElse 41\n",
      "subreddit_Drugs 1\n",
      "subreddit_Economics 27\n",
      "subreddit_Equality 4\n",
      "subreddit_Eve 2\n",
      "subreddit_FashionTechnology 1\n",
      "subreddit_Favors 4\n",
      "subreddit_FreeMicrosoftPoints 1\n",
      "subreddit_Frugal 20\n",
      "subreddit_GameDeals 3\n",
      "subreddit_Games 1\n",
      "subreddit_HappyBirthday 1\n",
      "subreddit_Health 1\n",
      "subreddit_Homebrewing 1\n",
      "subreddit_IAmA 224\n",
      "subreddit_ILiveIn 1\n",
      "subreddit_IndieGaming 4\n",
      "subreddit_Israel 1\n",
      "subreddit_JRPG 4\n",
      "subreddit_Libertarian 11\n",
      "subreddit_MW2 11\n",
      "subreddit_MapleLinks 8\n",
      "subreddit_Marijuana 74\n",
      "subreddit_MensRights 5\n",
      "subreddit_Music 12\n",
      "subreddit_NSFW_nospam 1\n",
      "subreddit_NewToTF2 1\n",
      "subreddit_News2Me 1\n",
      "subreddit_Ninjas 1\n",
      "subreddit_PS3 831\n",
      "subreddit_PSP 2\n",
      "subreddit_Pets 2\n",
      "subreddit_Portland 1\n",
      "subreddit_REDDITEXCHANGE 1\n",
      "subreddit_RedditizeME 1\n",
      "subreddit_RespectfulDebate 1\n",
      "subreddit_Rockband 1\n",
      "subreddit_SF4 2\n",
      "subreddit_Seattle 3\n",
      "subreddit_Socialize 1\n",
      "subreddit_SonyPS3 4\n",
      "subreddit_StreetFighter 1\n",
      "subreddit_SuicideWatch 4\n",
      "subreddit_TrueReddit 1\n",
      "subreddit_TwoXChromosomes 9\n",
      "subreddit_Ubuntu 3\n",
      "subreddit_WTF 278\n",
      "subreddit_WeAreTheMusicMakers 4\n",
      "subreddit_WebGames 10\n",
      "subreddit_WouldLikeToMeet 2\n",
      "subreddit_ads 3\n",
      "subreddit_anime 9\n",
      "subreddit_apple 49\n",
      "subreddit_artofpickup 1\n",
      "subreddit_ask 1\n",
      "subreddit_atheism 36\n",
      "subreddit_australia 10\n",
      "subreddit_auto 1\n",
      "subreddit_aww 2\n",
      "subreddit_bdsm 1\n",
      "subreddit_bestof 12\n",
      "subreddit_bestofcraigslist 1\n",
      "subreddit_bicycling 1\n",
      "subreddit_books 1\n",
      "subreddit_business 82\n",
      "subreddit_canada 13\n",
      "subreddit_chicago 1\n",
      "subreddit_circlejerk 1\n",
      "subreddit_coding 1\n",
      "subreddit_cogsci 1\n",
      "subreddit_collapse 1\n",
      "subreddit_comics 27\n",
      "subreddit_compsci 2\n",
      "subreddit_conspiracy 6\n",
      "subreddit_craigslist 3\n",
      "subreddit_crime 2\n",
      "subreddit_deadthings 1\n",
      "subreddit_dragonage 10\n",
      "subreddit_drunk 3\n",
      "subreddit_economy 3\n",
      "subreddit_electronicmusic 1\n",
      "subreddit_electronics 1\n",
      "subreddit_energy 2\n",
      "subreddit_entertainment 83\n",
      "subreddit_environment 22\n",
      "subreddit_fffffffuuuuuuuuuuuu 36\n",
      "subreddit_freeculture 1\n",
      "subreddit_funny 121\n",
      "subreddit_futureofreddit 2\n",
      "subreddit_gadgets 108\n",
      "subreddit_gamedev 6\n",
      "subreddit_gaming 7312\n",
      "subreddit_geek 82\n",
      "subreddit_giveaways 1\n",
      "subreddit_gonewild 1\n",
      "subreddit_guns 6\n",
      "subreddit_happy 7\n",
      "subreddit_hardware 32\n",
      "subreddit_history 1\n",
      "subreddit_hockey 1\n",
      "subreddit_hotblooded 1\n",
      "subreddit_howto 3\n",
      "subreddit_humor 1\n",
      "subreddit_ideasfortheadmins 3\n",
      "subreddit_iphone 2\n",
      "subreddit_islam 1\n",
      "subreddit_it 3\n",
      "subreddit_itookapicture 1\n",
      "subreddit_japan 1\n",
      "subreddit_johndollarfullofcrap 5\n",
      "subreddit_left4dead 4\n",
      "subreddit_lgbt 6\n",
      "subreddit_liberalarts 1\n",
      "subreddit_linux 133\n",
      "subreddit_linux4noobs 3\n",
      "subreddit_lisp 2\n",
      "subreddit_lists 3\n",
      "subreddit_lostgeneration 3\n",
      "subreddit_lovereddit 1\n",
      "subreddit_mac 1\n",
      "subreddit_math 1\n",
      "subreddit_meetup 1\n",
      "subreddit_microsoft 6\n",
      "subreddit_mod360 5\n",
      "subreddit_modhelp 2\n",
      "subreddit_motorcycles 1\n",
      "subreddit_movieclub 2\n",
      "subreddit_movies 20\n",
      "subreddit_needadvice 2\n",
      "subreddit_netflix 1\n",
      "subreddit_netsec 8\n",
      "subreddit_networking 3\n",
      "subreddit_news 7\n",
      "subreddit_nsfw 15\n",
      "subreddit_obama 1\n",
      "subreddit_offbeat 34\n",
      "subreddit_opendirectories 2\n",
      "subreddit_opensource 1\n",
      "subreddit_p2p 1\n",
      "subreddit_penisland 1\n",
      "subreddit_philosophy 2\n",
      "subreddit_photography 2\n",
      "subreddit_photoshop 1\n",
      "subreddit_pics 301\n",
      "subreddit_pinball 1\n",
      "subreddit_pittsburgh 2\n",
      "subreddit_poker 1\n",
      "subreddit_politics 157\n",
      "subreddit_programming 365\n",
      "subreddit_promos 26\n",
      "subreddit_psychology 1\n",
      "subreddit_reddit.com 1223\n",
      "subreddit_redditmakesagame 1\n",
      "subreddit_reddittraveljetblue 1\n",
      "subreddit_redheads 1\n",
      "subreddit_relationship_advice 5\n",
      "subreddit_religion 1\n",
      "subreddit_retrogames 2\n",
      "subreddit_rpg 3\n",
      "subreddit_ruby 2\n",
      "subreddit_scheme 1\n",
      "subreddit_science 91\n",
      "subreddit_scientology 1\n",
      "subreddit_scifi 11\n",
      "subreddit_secretsanta 16\n",
      "subreddit_self 41\n",
      "subreddit_sex 6\n",
      "subreddit_shittyadvice 2\n",
      "subreddit_skateboarding 1\n",
      "subreddit_soccer 4\n",
      "subreddit_socialism 1\n",
      "subreddit_software 6\n",
      "subreddit_somethingimade 1\n",
      "subreddit_space 1\n",
      "subreddit_sports 8\n",
      "subreddit_starcraft 1\n",
      "subreddit_startups 2\n",
      "subreddit_tech 3\n",
      "subreddit_technology 554\n",
      "subreddit_telescopes 1\n",
      "subreddit_tf2 50\n",
      "subreddit_tipofmytongue 4\n",
      "subreddit_todayilearned 7\n",
      "subreddit_torrents 1\n",
      "subreddit_trees 11\n",
      "subreddit_video 7\n",
      "subreddit_videos 44\n",
      "subreddit_wikipedia 4\n",
      "subreddit_women 1\n",
      "subreddit_worldnews 82\n",
      "subreddit_worldpolitics 2\n",
      "subreddit_worstof 1\n",
      "subreddit_wow 1\n",
      "subreddit_writing 2\n",
      "subreddit_xbmc 3\n",
      "subreddit_xbox360 145\n",
      "subreddit_xbox360games 4\n",
      "subreddit_xboxlive 38\n",
      "subreddit_xkcd 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(data=df, columns=['subreddit'])\n",
    "small_col = []\n",
    "total_col = 0\n",
    "for col in df.columns:\n",
    "    num_posts = df[col][df[col]==True].count()\n",
    "    print(col, num_posts)\n",
    "    total_col += 1\n",
    "    if num_posts <= 10:\n",
    "        small_col.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dummies for subreddit alone brought us from 6 columns up to 222 columns, 169 of which only have 10 mentions or less.  To avoid overfitting, I will be merging these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['score', 'body', 'PS', 'XBOX', 'naive_sentiment', 'bert_labels',\n",
      "       'bert_scores', 'subreddit_Android', 'subreddit_AskReddit',\n",
      "       'subreddit_DoesAnybodyElse', 'subreddit_Economics', 'subreddit_Frugal',\n",
      "       'subreddit_IAmA', 'subreddit_Libertarian', 'subreddit_MW2',\n",
      "       'subreddit_Marijuana', 'subreddit_Music', 'subreddit_PS3',\n",
      "       'subreddit_WTF', 'subreddit_apple', 'subreddit_atheism',\n",
      "       'subreddit_bestof', 'subreddit_business', 'subreddit_canada',\n",
      "       'subreddit_comics', 'subreddit_entertainment', 'subreddit_environment',\n",
      "       'subreddit_fffffffuuuuuuuuuuuu', 'subreddit_funny', 'subreddit_gadgets',\n",
      "       'subreddit_gaming', 'subreddit_geek', 'subreddit_hardware',\n",
      "       'subreddit_linux', 'subreddit_movies', 'subreddit_nsfw',\n",
      "       'subreddit_offbeat', 'subreddit_pics', 'subreddit_politics',\n",
      "       'subreddit_programming', 'subreddit_promos', 'subreddit_reddit.com',\n",
      "       'subreddit_science', 'subreddit_scifi', 'subreddit_secretsanta',\n",
      "       'subreddit_self', 'subreddit_technology', 'subreddit_tf2',\n",
      "       'subreddit_trees', 'subreddit_videos', 'subreddit_worldnews',\n",
      "       'subreddit_xbox360', 'subreddit_xboxlive', 'subreddit_other'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#leave out the non-subreddit columns so we don't delete them\n",
    "small_col.remove('body')\n",
    "small_col.remove('bert_scores')\n",
    "\n",
    "#row-wise aggregation of columns within small_col\n",
    "df['subreddit_other'] = df[small_col].aggregate('sum', axis=1)\n",
    "df = df.drop(columns=small_col)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54 columns is a lot more manageable.  Onto working with the body column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstring = ''\n",
    "\n",
    "for entry in df['body']:\n",
    "    superstring += entry\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "df_vocabulary = tokenizer.tokenize(superstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to tokenize and create dummies for bodies based off of most frequent words that aren't stopwords\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "vocab_series = pd.Series(df_vocabulary)\n",
    "print(vocab_series)\n",
    "print(vocab_series.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
