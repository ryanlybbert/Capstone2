{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14770 entries, c1e4o to c0i10ti\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   score             14770 non-null  int64         \n",
      " 1   controversiality  14770 non-null  int64         \n",
      " 2   subreddit         14770 non-null  object        \n",
      " 3   body              14770 non-null  object        \n",
      " 4   month             14770 non-null  int64         \n",
      " 5   year              14770 non-null  int64         \n",
      " 6   original_size     14770 non-null  int64         \n",
      " 7   PS                14770 non-null  int64         \n",
      " 8   XBOX              14770 non-null  int64         \n",
      " 9   PS_Count          14770 non-null  float64       \n",
      " 10  XBOX_Count        14770 non-null  float64       \n",
      " 11  date              14770 non-null  datetime64[ns]\n",
      " 12  naive_sentiment   14770 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int64(7), object(2)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import pipeline, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import os.path\n",
    "\n",
    "#! pip install nltk==3.3\n",
    "\n",
    "df = pd.read_json('dataframes/reddit_data.json')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we'll be needing a value to predict.  I'm going to be using a secondary sentiment analysis through a bert classifier that ranks the sentiment on a scale of 1-5 stars, as well as gives a confidence score.  Since this is a large file, I've broken up the data into parts, saved them, and recombined them after adding the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes/df06_bert.json exists\n",
      "dataframes/df07_bert.json exists\n",
      "dataframes/df08_bert.json exists\n",
      "dataframes/df09_bert.json exists\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "max_length = 512\n",
    "\n",
    "years = ['06', '07', '08', '09']\n",
    "\n",
    "for year in years:\n",
    "    if os.path.exists('dataframes/df'+year+'_bert.json'):\n",
    "        print('dataframes/df'+year+'_bert.json exists')\n",
    "    else:\n",
    "        labels = []\n",
    "        scores = []\n",
    "        mini_df = df[df['year']==2000+int(year)]\n",
    "        for post in tqdm(mini_df['body']):\n",
    "            a = classifier(post, max_length=max_length, truncation=True) #this returns a 1-element list of a dictionary\n",
    "            labels.append(a[0]['label'][0]) #a[0]['label'] returns a label between '1 star' and '5 stars', I only need the integer\n",
    "            scores.append(a[0]['score']) #confidence score of said label\n",
    "        mini_df['bert_labels'] = labels\n",
    "        mini_df['bert_scores'] = scores\n",
    "        mini_df.to_json('dataframes/df'+year+'_bert.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14770 entries, c1e4o to c0i10ti\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   score             14770 non-null  int64         \n",
      " 1   controversiality  14770 non-null  int64         \n",
      " 2   subreddit         14770 non-null  object        \n",
      " 3   body              14770 non-null  object        \n",
      " 4   month             14770 non-null  int64         \n",
      " 5   year              14770 non-null  int64         \n",
      " 6   original_size     14770 non-null  int64         \n",
      " 7   PS                14770 non-null  int64         \n",
      " 8   XBOX              14770 non-null  int64         \n",
      " 9   PS_Count          14770 non-null  float64       \n",
      " 10  XBOX_Count        14770 non-null  float64       \n",
      " 11  date              14770 non-null  datetime64[ns]\n",
      " 12  naive_sentiment   14770 non-null  float64       \n",
      " 13  bert_labels       14770 non-null  int64         \n",
      " 14  bert_scores       14770 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(4), int64(8), object(2)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#recombining data after the split.\n",
    "df06 = pd.read_json('dataframes/df06_bert.json')\n",
    "df07 = pd.read_json('dataframes/df07_bert.json')\n",
    "df08 = pd.read_json('dataframes/df08_bert.json')\n",
    "df09 = pd.read_json('dataframes/df09_bert.json')\n",
    "\n",
    "df = df06.append(df07.append(df08.append(df09)))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that's out of the way I need to decide which columns will be useful for my model.\n",
    "\n",
    "score should have an impact, though controversiality might not since it's effectively a boolean value with a large majority of values being equal to 0.\n",
    "body will definitely make an impact, we'll need to make some serious changes to make it numeric.\n",
    "month, year, and date are mostly for the time series I made previously, and will not add much.\n",
    "original_size, PS_Count, and XBOX_Count were columns I added for plotting, and have no purpose here.\n",
    "naive_sentiment is definitely important.\n",
    "PS, XBOX, and subreddit are categoricals that should aid in predictions.\n",
    "bert_labels are going to be the prediction value and bert_scores will supplement that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14770 entries, c1e4o to c0i10ti\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   score            14770 non-null  int64  \n",
      " 1   subreddit        14770 non-null  object \n",
      " 2   body             14770 non-null  object \n",
      " 3   PS               14770 non-null  int64  \n",
      " 4   XBOX             14770 non-null  int64  \n",
      " 5   naive_sentiment  14770 non-null  float64\n",
      " 6   bert_labels      14770 non-null  int64  \n",
      " 7   bert_scores      14770 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['PS_Count', 'controversiality', 'XBOX_Count', 'original_size', 'month', 'year', 'date'], inplace=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to address the score column's outliers.  Half of all values are between 0 and 3, but the overall range is from -45 to 398.  The graph from the EDA shows that the variance has been increasing over time and has a non-normal distribution.  If we only keep rows between -30 and 30, we only lose 161 columns in the process, and in doing so make score a more reliable predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              score            PS          XBOX  naive_sentiment  \\\n",
      "count  14609.000000  14609.000000  14609.000000     14609.000000   \n",
      "mean       2.132453      0.448080      0.610446         0.077496   \n",
      "std        4.001580      0.497314      0.487666         0.221732   \n",
      "min      -30.000000      0.000000      0.000000        -1.000000   \n",
      "25%        1.000000      0.000000      0.000000        -0.006667   \n",
      "50%        1.000000      0.000000      1.000000         0.056481   \n",
      "75%        3.000000      1.000000      1.000000         0.194924   \n",
      "max       30.000000      1.000000      1.000000         1.000000   \n",
      "\n",
      "        bert_labels   bert_scores  \n",
      "count  14609.000000  14609.000000  \n",
      "mean       2.486412      0.436582  \n",
      "std        1.465876      0.145641  \n",
      "min        1.000000      0.208104  \n",
      "25%        1.000000      0.329384  \n",
      "50%        2.000000      0.401645  \n",
      "75%        4.000000      0.508862  \n",
      "max        5.000000      0.980305  \n"
     ]
    }
   ],
   "source": [
    "df.drop(df[(df['score'] > 30) | (df['score'] < -30)].index, inplace=True)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, this dropped the std of score from 11.1 to 4.001, while maintaining the percentiles and only changing the mean by about 0.8.  The last thing we need to address is the body and subreddit columns.  We should be able to create dummies for subreddit just fine, but the sheer number of unique words in body would add around 19,000 columns to the database as we'll see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 5842\n",
      "body 0\n",
      "PS 6546\n",
      "XBOX 8918\n",
      "naive_sentiment 51\n",
      "bert_labels 5671\n",
      "bert_scores 0\n",
      "subreddit_4chan 1\n",
      "subreddit_AmericanGovernment 1\n",
      "subreddit_AmericanPolitics 1\n",
      "subreddit_Anarchism 5\n",
      "subreddit_Android 41\n",
      "subreddit_Art 1\n",
      "subreddit_AskReddit 1250\n",
      "subreddit_Astronomy 1\n",
      "subreddit_BDSMcommunity 1\n",
      "subreddit_Baking 1\n",
      "subreddit_Borderlands 6\n",
      "subreddit_Christianity 2\n",
      "subreddit_CommonLaw 2\n",
      "subreddit_DAE 2\n",
      "subreddit_DIY 2\n",
      "subreddit_Design 1\n",
      "subreddit_DoesAnybodyElse 41\n",
      "subreddit_Drugs 1\n",
      "subreddit_Economics 27\n",
      "subreddit_Equality 4\n",
      "subreddit_Eve 2\n",
      "subreddit_FashionTechnology 1\n",
      "subreddit_Favors 4\n",
      "subreddit_FreeMicrosoftPoints 1\n",
      "subreddit_Frugal 20\n",
      "subreddit_GameDeals 3\n",
      "subreddit_Games 1\n",
      "subreddit_HappyBirthday 1\n",
      "subreddit_Health 1\n",
      "subreddit_Homebrewing 1\n",
      "subreddit_IAmA 224\n",
      "subreddit_ILiveIn 1\n",
      "subreddit_IndieGaming 4\n",
      "subreddit_Israel 1\n",
      "subreddit_JRPG 4\n",
      "subreddit_Libertarian 11\n",
      "subreddit_MW2 11\n",
      "subreddit_MapleLinks 8\n",
      "subreddit_Marijuana 74\n",
      "subreddit_MensRights 5\n",
      "subreddit_Music 12\n",
      "subreddit_NSFW_nospam 1\n",
      "subreddit_NewToTF2 1\n",
      "subreddit_News2Me 1\n",
      "subreddit_Ninjas 1\n",
      "subreddit_PS3 831\n",
      "subreddit_PSP 2\n",
      "subreddit_Pets 2\n",
      "subreddit_Portland 1\n",
      "subreddit_REDDITEXCHANGE 1\n",
      "subreddit_RedditizeME 1\n",
      "subreddit_RespectfulDebate 1\n",
      "subreddit_Rockband 1\n",
      "subreddit_SF4 2\n",
      "subreddit_Seattle 3\n",
      "subreddit_Socialize 1\n",
      "subreddit_SonyPS3 4\n",
      "subreddit_StreetFighter 1\n",
      "subreddit_SuicideWatch 4\n",
      "subreddit_TrueReddit 1\n",
      "subreddit_TwoXChromosomes 9\n",
      "subreddit_Ubuntu 3\n",
      "subreddit_WTF 278\n",
      "subreddit_WeAreTheMusicMakers 4\n",
      "subreddit_WebGames 10\n",
      "subreddit_WouldLikeToMeet 2\n",
      "subreddit_ads 3\n",
      "subreddit_anime 9\n",
      "subreddit_apple 49\n",
      "subreddit_artofpickup 1\n",
      "subreddit_ask 1\n",
      "subreddit_atheism 36\n",
      "subreddit_australia 10\n",
      "subreddit_auto 1\n",
      "subreddit_aww 2\n",
      "subreddit_bdsm 1\n",
      "subreddit_bestof 12\n",
      "subreddit_bestofcraigslist 1\n",
      "subreddit_bicycling 1\n",
      "subreddit_books 1\n",
      "subreddit_business 82\n",
      "subreddit_canada 13\n",
      "subreddit_chicago 1\n",
      "subreddit_circlejerk 1\n",
      "subreddit_coding 1\n",
      "subreddit_cogsci 1\n",
      "subreddit_collapse 1\n",
      "subreddit_comics 27\n",
      "subreddit_compsci 2\n",
      "subreddit_conspiracy 6\n",
      "subreddit_craigslist 3\n",
      "subreddit_crime 2\n",
      "subreddit_deadthings 1\n",
      "subreddit_dragonage 10\n",
      "subreddit_drunk 3\n",
      "subreddit_economy 3\n",
      "subreddit_electronicmusic 1\n",
      "subreddit_electronics 1\n",
      "subreddit_energy 2\n",
      "subreddit_entertainment 83\n",
      "subreddit_environment 22\n",
      "subreddit_fffffffuuuuuuuuuuuu 36\n",
      "subreddit_freeculture 1\n",
      "subreddit_funny 121\n",
      "subreddit_futureofreddit 2\n",
      "subreddit_gadgets 108\n",
      "subreddit_gamedev 6\n",
      "subreddit_gaming 7312\n",
      "subreddit_geek 82\n",
      "subreddit_giveaways 1\n",
      "subreddit_gonewild 1\n",
      "subreddit_guns 6\n",
      "subreddit_happy 7\n",
      "subreddit_hardware 32\n",
      "subreddit_history 1\n",
      "subreddit_hockey 1\n",
      "subreddit_hotblooded 1\n",
      "subreddit_howto 3\n",
      "subreddit_humor 1\n",
      "subreddit_ideasfortheadmins 3\n",
      "subreddit_iphone 2\n",
      "subreddit_islam 1\n",
      "subreddit_it 3\n",
      "subreddit_itookapicture 1\n",
      "subreddit_japan 1\n",
      "subreddit_johndollarfullofcrap 5\n",
      "subreddit_left4dead 4\n",
      "subreddit_lgbt 6\n",
      "subreddit_liberalarts 1\n",
      "subreddit_linux 133\n",
      "subreddit_linux4noobs 3\n",
      "subreddit_lisp 2\n",
      "subreddit_lists 3\n",
      "subreddit_lostgeneration 3\n",
      "subreddit_lovereddit 1\n",
      "subreddit_mac 1\n",
      "subreddit_math 1\n",
      "subreddit_meetup 1\n",
      "subreddit_microsoft 6\n",
      "subreddit_mod360 5\n",
      "subreddit_modhelp 2\n",
      "subreddit_motorcycles 1\n",
      "subreddit_movieclub 2\n",
      "subreddit_movies 20\n",
      "subreddit_needadvice 2\n",
      "subreddit_netflix 1\n",
      "subreddit_netsec 8\n",
      "subreddit_networking 3\n",
      "subreddit_news 7\n",
      "subreddit_nsfw 15\n",
      "subreddit_obama 1\n",
      "subreddit_offbeat 34\n",
      "subreddit_opendirectories 2\n",
      "subreddit_opensource 1\n",
      "subreddit_p2p 1\n",
      "subreddit_penisland 1\n",
      "subreddit_philosophy 2\n",
      "subreddit_photography 2\n",
      "subreddit_photoshop 1\n",
      "subreddit_pics 301\n",
      "subreddit_pinball 1\n",
      "subreddit_pittsburgh 2\n",
      "subreddit_poker 1\n",
      "subreddit_politics 157\n",
      "subreddit_programming 365\n",
      "subreddit_promos 26\n",
      "subreddit_psychology 1\n",
      "subreddit_reddit.com 1223\n",
      "subreddit_redditmakesagame 1\n",
      "subreddit_reddittraveljetblue 1\n",
      "subreddit_redheads 1\n",
      "subreddit_relationship_advice 5\n",
      "subreddit_religion 1\n",
      "subreddit_retrogames 2\n",
      "subreddit_rpg 3\n",
      "subreddit_ruby 2\n",
      "subreddit_scheme 1\n",
      "subreddit_science 91\n",
      "subreddit_scientology 1\n",
      "subreddit_scifi 11\n",
      "subreddit_secretsanta 16\n",
      "subreddit_self 41\n",
      "subreddit_sex 6\n",
      "subreddit_shittyadvice 2\n",
      "subreddit_skateboarding 1\n",
      "subreddit_soccer 4\n",
      "subreddit_socialism 1\n",
      "subreddit_software 6\n",
      "subreddit_somethingimade 1\n",
      "subreddit_space 1\n",
      "subreddit_sports 8\n",
      "subreddit_starcraft 1\n",
      "subreddit_startups 2\n",
      "subreddit_tech 3\n",
      "subreddit_technology 554\n",
      "subreddit_telescopes 1\n",
      "subreddit_tf2 50\n",
      "subreddit_tipofmytongue 4\n",
      "subreddit_todayilearned 7\n",
      "subreddit_torrents 1\n",
      "subreddit_trees 11\n",
      "subreddit_video 7\n",
      "subreddit_videos 44\n",
      "subreddit_wikipedia 4\n",
      "subreddit_women 1\n",
      "subreddit_worldnews 82\n",
      "subreddit_worldpolitics 2\n",
      "subreddit_worstof 1\n",
      "subreddit_wow 1\n",
      "subreddit_writing 2\n",
      "subreddit_xbmc 3\n",
      "subreddit_xbox360 145\n",
      "subreddit_xbox360games 4\n",
      "subreddit_xboxlive 38\n",
      "subreddit_xkcd 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(data=df, columns=['subreddit']) #consider substituting using value count before dummies\n",
    "small_col = []\n",
    "total_col = 0\n",
    "for col in df.columns:\n",
    "    num_posts = df[col][df[col]==True].count()\n",
    "    print(col, num_posts)\n",
    "    total_col += 1\n",
    "    if num_posts <= 50: #potentially pick bigger number\n",
    "        small_col.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dummies for subreddit alone brought us from 6 columns up to 222 columns, most of which have 50 mentions or less.  To avoid overfitting, I will be merging these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['score', 'body', 'PS', 'XBOX', 'naive_sentiment', 'bert_labels',\n",
      "       'bert_scores', 'subreddit_AskReddit', 'subreddit_IAmA',\n",
      "       'subreddit_Marijuana', 'subreddit_PS3', 'subreddit_WTF',\n",
      "       'subreddit_business', 'subreddit_entertainment', 'subreddit_funny',\n",
      "       'subreddit_gadgets', 'subreddit_gaming', 'subreddit_geek',\n",
      "       'subreddit_linux', 'subreddit_pics', 'subreddit_politics',\n",
      "       'subreddit_programming', 'subreddit_reddit.com', 'subreddit_science',\n",
      "       'subreddit_technology', 'subreddit_worldnews', 'subreddit_xbox360',\n",
      "       'subreddit_other'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#leave out the non-subreddit columns so we don't delete them\n",
    "small_col.remove('body')\n",
    "small_col.remove('bert_scores')\n",
    "\n",
    "#row-wise aggregation of columns within small_col\n",
    "df['subreddit_other'] = df[small_col].aggregate('sum', axis=1)\n",
    "df = df.drop(columns=small_col)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 columns is a lot more manageable, especially since we'll be adding more soon.  Onto working with the body column.  First we need to reduce the body section into tokens, rather than paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'article', 'raises', 'concerns', 'that', 'a', 'single', 'speed', 'blur', '##ay', 'drive', 'will', 'be', 'too', 'slow', 'but', 'no', 'one', 'has', 'even', 'announced', 'a', 'single', 'speed', 'blur', '##ay', 'drive', 'most', 'of', 'them', 'are', '4', '##x', 'so', 'far', 'since', 'there', 'aren', '##t', 'even', 'sing', '##el', 'speed', 'drives', 'available', 'it', 'seems', 'unlikely', 'sony', 'would']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing\n",
    "superstring = ''\n",
    "\n",
    "for entry in df['body']:\n",
    "    superstring += entry\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocabulary = tokenizer.tokenize(superstring)\n",
    "\n",
    "print(vocabulary[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to remove stopwords and reduce words to their base parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing and removing stopwords\n",
    "def lemma(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    line = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        #word = re.sub(regex,'', word) #if I want to get rid of words within a certain regex, like urls\n",
    "        \n",
    "        if tag.startswith('NN'): #labels words as a noun\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'): #labels words as a verb\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        \n",
    "        if len(word) > 0 and word not in string.punctuation and word.lower() not in stop_words:\n",
    "            line.append(lemmatizer.lemmatize(word, pos))\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article', 'raise', 'concern', 'single', 'speed', 'blur', '##ay', 'drive', 'slow', 'one', 'even', 'announce', 'single', 'speed', 'blur', '##ay', 'drive', '4', '##x', 'far', 'since', '##t', 'even', 'sing', '##el', 'speed', 'drive', 'available', 'seem', 'unlikely', 'sony', 'would', 'use', 'come', 'cost', 'large', 'part', 'investment', 'already', 'make', 'fa', '##bs', 'develop', 'cell', 'processor', 'together', 'ibm', '##shi', '##ba', 'sony']\n",
      "xbox    11980\n",
      "game    11322\n",
      "##t     10410\n",
      "##s     10274\n",
      "p        8182\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tokens = lemma(vocabulary)\n",
    "token_series = pd.Series(tokens)\n",
    "token_series = token_series.value_counts()\n",
    "print(tokens[0:50])\n",
    "print(token_series.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide on which words we will make columns out of, I'm going to use a Term Frequency / Inverse Document Frequency model to score words based off of how frequent they are in both individual posts and the database as a whole.  We'll need the IDF for each token first.  The IDF is basically a measure of how rare a word is in the database, and as a result words with high IDF's have a stronger influence on the features made from each paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate IDF\n",
    "for i in df.index: #need to reduce body column into a string of its tokens\n",
    "    entry = df.loc[i]['body']\n",
    "    body_tokens = tokenizer.tokenize(entry)\n",
    "    body_lemma = lemma(body_tokens)\n",
    "    restring = ''\n",
    "    for token in body_lemma:\n",
    "        restring += token + ' '\n",
    "    df.loc[i, 'body'] = restring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15780/15780 [02:32<00:00, 103.43it/s]\n"
     ]
    }
   ],
   "source": [
    "IDF_list = []\n",
    "total_posts = df['body'].count()\n",
    "\n",
    "for token in tqdm(token_series.index):\n",
    "    total_posts_with_token = df['body'][df['body'].str.contains(token)].count()\n",
    "    if total_posts_with_token > 1:\n",
    "        IDF = np.log(total_posts / total_posts_with_token)\n",
    "        IDF_list.append([token, IDF])\n",
    "        \n",
    "TF_IDF_df = pd.DataFrame(IDF_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             token       IDF\n",
      "10619    inspector  8.896246\n",
      "10989   contractor  8.896246\n",
      "10977       ##iger  8.896246\n",
      "10978    endurance  8.896246\n",
      "10979      postage  8.896246\n",
      "10980       ##rave  8.896246\n",
      "10981        paula  8.896246\n",
      "10982  prehistoric  8.896246\n",
      "10983       ##sier  8.896246\n",
      "10984        ##ans  8.896246\n",
      "10985   algorithms  8.896246\n",
      "10986        scots  8.896246\n",
      "10987        ##wen  8.896246\n",
      "4259       britney  8.896246\n",
      "10990         lots  8.896246\n",
      "10974       ##mple  8.896246\n",
      "10991       ##udes  8.896246\n",
      "10992      malcolm  8.896246\n",
      "10993  psychiatric  8.896246\n",
      "10994      eclipse  8.896246\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_df.rename(columns={0:'token', 1:'IDF'}, inplace=True)\n",
    "print(TF_IDF_df.sort_values(by='IDF', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the IDF for each token calculated, time to work on the TF for each post.  The TF measures how frequently each token occurs within one post, and is multiplied with the IDF to generate the importance of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_calc (TF_df, post, total_posts):\n",
    "    TF_IDF_list = []\n",
    "    \n",
    "    for i in TF_df.index:\n",
    "        token = TF_df.loc[i]['token']\n",
    "        IDF = TF_df.loc[i]['IDF']\n",
    "        token_count_in_post = post.count(token)\n",
    "        \n",
    "        if token_count_in_post > 0:\n",
    "            TF = total_posts / token_count_in_post\n",
    "        else:\n",
    "            TF = 0\n",
    "        TF_IDF_list.append([TF * IDF])\n",
    "        \n",
    "    return pd.DataFrame(TF_IDF_list, columns=['TFIDF calc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note is that mapping the full TF-IDF for my database requires 9k x 16k cells, or about 144 million.  To make this easier on my computer, I'm instead only saving the total score for each body and the average TF-IDF of each token, the former will be made into a new column for the main dataframe and the latter will be used to determine which tokens should be used as prediction variables.  Breaking up the loops so I can save progress as I go, since this takes a few hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes/TFIDF_0.json exists\n",
      "dataframes/TFIDF_1000.json exists\n",
      "dataframes/TFIDF_2000.json exists\n",
      "dataframes/TFIDF_3000.json exists\n",
      "dataframes/TFIDF_4000.json exists\n",
      "dataframes/TFIDF_5000.json exists\n",
      "dataframes/TFIDF_6000.json exists\n",
      "dataframes/TFIDF_7000.json exists\n",
      "dataframes/TFIDF_8000.json exists\n",
      "dataframes/TFIDF_9000.json exists\n",
      "dataframes/TFIDF_10000.json exists\n",
      "dataframes/TFIDF_11000.json exists\n",
      "dataframes/TFIDF_12000.json exists\n",
      "dataframes/TFIDF_13000.json exists\n",
      "dataframes/TFIDF_14000.json exists\n"
     ]
    }
   ],
   "source": [
    "loop = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000]\n",
    "\n",
    "#do 1000 at a time, saving as I go for time\n",
    "for k in loop:\n",
    "    if os.path.exists('dataframes/TFIDF_'+str(k)+'.json'):\n",
    "        print('dataframes/TFIDF_'+str(k)+'.json exists')\n",
    "    else:\n",
    "        TF_IDF_df['TF_ave'] = 0\n",
    "        body_tfidf_score = []\n",
    "        if k == 14000:\n",
    "            x=df['body'].count() - k\n",
    "        else:\n",
    "            x=1000\n",
    "        \n",
    "        for i in tqdm(range(x)):\n",
    "            TF_to_add = TF_calc(TF_IDF_df, df['body'].iloc[i+k], total_posts)\n",
    "            TF_IDF_df['TF_ave'] += (TF_to_add['TFIDF calc'] / df['body'].count())\n",
    "            body_tfidf_score.append(TF_to_add['TFIDF calc'].sum())\n",
    "        TF_IDF_df = TF_IDF_df[TF_IDF_df['TF_ave'] > 0]\n",
    "        TF_IDF_df.to_json('dataframes/TFIDF_'+str(k)+'.json')\n",
    "        pd.DataFrame(body_tfidf_score).to_json('dataframes/body_TFIDF_'+str(k)+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave0 = pd.read_json('dataframes/TFIDF_0.json')\n",
    "ave1 = pd.read_json('dataframes/TFIDF_1000.json')\n",
    "ave2 = pd.read_json('dataframes/TFIDF_2000.json')\n",
    "ave3 = pd.read_json('dataframes/TFIDF_3000.json')\n",
    "ave4 = pd.read_json('dataframes/TFIDF_4000.json')\n",
    "ave5 = pd.read_json('dataframes/TFIDF_5000.json')\n",
    "ave6 = pd.read_json('dataframes/TFIDF_6000.json')\n",
    "ave7 = pd.read_json('dataframes/TFIDF_7000.json')\n",
    "ave8 = pd.read_json('dataframes/TFIDF_8000.json')\n",
    "ave9 = pd.read_json('dataframes/TFIDF_9000.json')\n",
    "ave10 = pd.read_json('dataframes/TFIDF_10000.json')\n",
    "ave11 = pd.read_json('dataframes/TFIDF_11000.json')\n",
    "ave12 = pd.read_json('dataframes/TFIDF_12000.json')\n",
    "ave13 = pd.read_json('dataframes/TFIDF_13000.json')\n",
    "ave14 = pd.read_json('dataframes/TFIDF_14000.json')\n",
    "\n",
    "TF_IDF_df = ave0.append(ave1.append(ave2.append(ave3.append(ave4.append(ave5.append(ave6.append(ave7.append(ave8.append(ave9.append(ave10.append(ave11.append(ave12.append(ave13.append(ave14))))))))))))))\n",
    "\n",
    "body0 = pd.read_json('dataframes/body_TFIDF_0.json')\n",
    "body1 = pd.read_json('dataframes/body_TFIDF_1000.json')\n",
    "body2 = pd.read_json('dataframes/body_TFIDF_2000.json')\n",
    "body3 = pd.read_json('dataframes/body_TFIDF_3000.json')\n",
    "body4 = pd.read_json('dataframes/body_TFIDF_4000.json')\n",
    "body5 = pd.read_json('dataframes/body_TFIDF_5000.json')\n",
    "body6 = pd.read_json('dataframes/body_TFIDF_6000.json')\n",
    "body7 = pd.read_json('dataframes/body_TFIDF_7000.json')\n",
    "body8 = pd.read_json('dataframes/body_TFIDF_8000.json')\n",
    "body9 = pd.read_json('dataframes/body_TFIDF_9000.json')\n",
    "body10 = pd.read_json('dataframes/body_TFIDF_10000.json')\n",
    "body11 = pd.read_json('dataframes/body_TFIDF_11000.json')\n",
    "body12 = pd.read_json('dataframes/body_TFIDF_12000.json')\n",
    "body13 = pd.read_json('dataframes/body_TFIDF_13000.json')\n",
    "body14 = pd.read_json('dataframes/body_TFIDF_14000.json')\n",
    "\n",
    "body_scores = body0.append(body1.append(body2.append(body3.append(body4.append(body5.append(body6.append(body7.append(body8.append(body9.append(body10.append(body11.append(body12.append(body13.append(body14))))))))))))))\n",
    "\n",
    "TF_IDF_df = TF_IDF_df.reset_index(drop=True)\n",
    "body_scores = body_scores.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to add the body_score column to the main df and make token count columns for the tokens with the highest average 50 TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18506       360\n",
      "19988    budget\n",
      "18827        60\n",
      "27650      live\n",
      "4063       stat\n",
      "          ...  \n",
      "22075        im\n",
      "1892         ai\n",
      "28720      2006\n",
      "6960          4\n",
      "9127         jo\n",
      "Name: token, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print out top tokens\n",
    "important_tokens = TF_IDF_df[TF_IDF_df['TF_ave'] > 0].sort_values(by='TF_ave', ascending=False).head(100)\n",
    "print(important_tokens['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       score                                               body  PS  XBOX  \\\n",
      "c1e4o      7  article raise concern single speed blur ##ay d...   0     1   \n",
      "c3kh6      2  link ##jack http www mac ##world com 2005 09 f...   1     0   \n",
      "c5iqp      7  feel discover obligation buy playstation ##s i...   1     0   \n",
      "c5vr3      3  gt make nintendo wii special different xbox 36...   1     1   \n",
      "c5zhh     -9  ##t know fa ##gs iv ##e never excited gaming i...   0     1   \n",
      "\n",
      "       naive_sentiment  bert_labels  bert_scores  subreddit_AskReddit  \\\n",
      "c1e4o        -0.055082            2     0.353368                    0   \n",
      "c3kh6         0.000000            1     0.289909                    0   \n",
      "c5iqp         0.235417            2     0.332737                    0   \n",
      "c5vr3         0.110119            3     0.320127                    0   \n",
      "c5zhh         0.418750            3     0.303217                    0   \n",
      "\n",
      "       subreddit_IAmA  subreddit_Marijuana  ...  police  op  ap  photo  ct  \\\n",
      "c1e4o               0                    0  ...       0   1   0      0   0   \n",
      "c3kh6               0                    0  ...       0   0   0      0   0   \n",
      "c5iqp               0                    0  ...       0   1   0      0   0   \n",
      "c5vr3               0                    0  ...       0   0   0      0   0   \n",
      "c5zhh               0                    0  ...       0   0   0      0   0   \n",
      "\n",
      "       im  ai  2006  4  jo  \n",
      "c1e4o   0   3     0  1   0  \n",
      "c3kh6   0   0     0  0   0  \n",
      "c5iqp   1   1     0  0   0  \n",
      "c5vr3   0   0     0  0   1  \n",
      "c5zhh   0   0     0  0   0  \n",
      "\n",
      "[5 rows x 103 columns]\n"
     ]
    }
   ],
   "source": [
    "#make a column out of the body scores\n",
    "df['body_score'] = 0\n",
    "\n",
    "for i in range(df['body'].count()):\n",
    "    df['body_score'].iloc[i] = body_scores[0].iloc[i]\n",
    "\n",
    "#make columns out of the most important tokens\n",
    "for token in important_tokens['token']:\n",
    "    token_list = []\n",
    "    df[token] = 0\n",
    "    \n",
    "    for j in df.index:\n",
    "        token_list.append(df.loc[j]['body'].count(token))\n",
    "        \n",
    "    token_column = pd.DataFrame(token_list)\n",
    "    \n",
    "    k = 0\n",
    "    for i in df.index: #doing it this way because of setting-on-a-copy issues with iloc on df\n",
    "        df.loc[i, token] = token_column.iloc[k][0]\n",
    "        k += 1\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is properly preprocessed, it's time to sending it off to be modeled in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['body', 'naive_sentiment', 'bert_scores'])\n",
    "df.to_json('dataframes/preprocessed_df.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
